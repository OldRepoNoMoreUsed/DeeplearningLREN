---
backprop: true
backpropType: "Standard"
confs:
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<convolution>
    activationFn: !<ReLU> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    cudnnAlgoMode: "PREFER_FASTEST"
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 5
    - 5
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Convolution layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    nin: 1
    nout: 20
    padding:
    - 0
    - 0
    rho: NaN
    rmsDecay: NaN
    stride:
    - 1
    - 1
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<subsampling>
    activationFn: !<Sigmoid> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    dist: null
    dropOut: 0.0
    eps: 1.0E-8
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 2
    - 2
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Max pooling layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    padding:
    - 0
    - 0
    pnorm: 0
    poolingType: "MAX"
    rho: NaN
    rmsDecay: NaN
    stride:
    - 2
    - 2
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<convolution>
    activationFn: !<ReLU> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    cudnnAlgoMode: "PREFER_FASTEST"
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 1
    - 1
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Convolution layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    nin: 20
    nout: 20
    padding:
    - 2
    - 2
    rho: NaN
    rmsDecay: NaN
    stride:
    - 1
    - 1
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<subsampling>
    activationFn: !<Sigmoid> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    dist: null
    dropOut: 0.0
    eps: 1.0E-8
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 2
    - 2
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Max pooling layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    padding:
    - 0
    - 0
    pnorm: 0
    poolingType: "MAX"
    rho: NaN
    rmsDecay: NaN
    stride:
    - 2
    - 2
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<convolution>
    activationFn: !<ReLU> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    cudnnAlgoMode: "PREFER_FASTEST"
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 1
    - 1
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Convolution layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    nin: 20
    nout: 20
    padding:
    - 2
    - 2
    rho: NaN
    rmsDecay: NaN
    stride:
    - 1
    - 1
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<subsampling>
    activationFn: !<Sigmoid> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    dist: null
    dropOut: 0.0
    eps: 1.0E-8
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 2
    - 2
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Max pooling layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    padding:
    - 0
    - 0
    pnorm: 0
    poolingType: "MAX"
    rho: NaN
    rmsDecay: NaN
    stride:
    - 2
    - 2
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<convolution>
    activationFn: !<ReLU> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    cudnnAlgoMode: "PREFER_FASTEST"
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 1
    - 1
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Convolution layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    nin: 20
    nout: 20
    padding:
    - 2
    - 2
    rho: NaN
    rmsDecay: NaN
    stride:
    - 1
    - 1
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<subsampling>
    activationFn: !<Sigmoid> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    convolutionMode: "Truncate"
    dist: null
    dropOut: 0.0
    eps: 1.0E-8
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    kernelSize:
    - 2
    - 2
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "Max pooling layer"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    padding:
    - 0
    - 0
    pnorm: 0
    poolingType: "MAX"
    rho: NaN
    rmsDecay: NaN
    stride:
    - 2
    - 2
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<dense>
    activationFn: !<ReLU> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "layer8"
    learningRate: 0.001
    learningRateSchedule: null
    momentum: 0.9
    momentumSchedule: {}
    nin: 84500
    nout: 180
    rho: NaN
    rmsDecay: NaN
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
- iterationCount: 0
  l1ByParam:
    b: 0.0
    W: 0.0
  l2ByParam:
    b: 0.0
    W: 4.0E-4
  layer: !<output>
    activationFn: !<Softmax> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.001
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    l1: 0.0
    l1Bias: 0.0
    l2: 4.0E-4
    l2Bias: 0.0
    layerName: "layer9"
    learningRate: 0.001
    learningRateSchedule: null
    lossFn: !<NegativeLogLikelihood> {}
    lossFunction: "NEGATIVELOGLIKELIHOOD"
    momentum: 0.9
    momentumSchedule: {}
    nin: 180
    nout: 2
    rho: NaN
    rmsDecay: NaN
    updater: "NESTEROVS"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam:
    b: 0.001
    W: 0.001
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 12345
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables:
  - "W"
  - "b"
inputPreProcessors:
  8: !<cnnToFeedForward>
    inputHeight: 65
    inputWidth: 65
    numChannels: 20
iterationCount: 1164
pretrain: false
tbpttBackLength: 20
tbpttFwdLength: 20
